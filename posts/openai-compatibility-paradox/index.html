<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The OpenAI Compatibility Paradox | deepankar.log</title><meta name=keywords content="LLM,OpenAI,Anthropic,Gemini,Completions API"><meta name=description content="Why the LLM API space needs a true standard"><meta name=author content="Deepankar Mahapatro"><link rel=canonical href=https://deepankarm.github.io/posts/openai-compatibility-paradox/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://deepankarm.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://deepankarm.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://deepankarm.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://deepankarm.github.io/apple-touch-icon.png><link rel=mask-icon href=https://deepankarm.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://deepankarm.github.io/posts/openai-compatibility-paradox/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "09cbd7674917411283ca3c61ea31135d"}'></script><meta property="og:url" content="https://deepankarm.github.io/posts/openai-compatibility-paradox/"><meta property="og:site_name" content="deepankar.log"><meta property="og:title" content="The OpenAI Compatibility Paradox"><meta property="og:description" content="Why the LLM API space needs a true standard"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-17T18:00:00+05:30"><meta property="article:modified_time" content="2025-12-17T18:00:00+05:30"><meta property="article:tag" content="LLM"><meta property="article:tag" content="OpenAI"><meta property="article:tag" content="Anthropic"><meta property="article:tag" content="Gemini"><meta property="article:tag" content="Completions API"><meta name=twitter:card content="summary"><meta name=twitter:title content="The OpenAI Compatibility Paradox"><meta name=twitter:description content="Why the LLM API space needs a true standard"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://deepankarm.github.io/posts/"},{"@type":"ListItem","position":2,"name":"The OpenAI Compatibility Paradox","item":"https://deepankarm.github.io/posts/openai-compatibility-paradox/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The OpenAI Compatibility Paradox","name":"The OpenAI Compatibility Paradox","description":"Why the LLM API space needs a true standard","keywords":["LLM","OpenAI","Anthropic","Gemini","Completions API"],"articleBody":"The promise of a standardized interface for LLMs via OpenAI-compatible endpoints is compelling. In theory, it allows for a plug-and-play architecture where switching models is as trivial as changing a base_url. In practice, this compatibility is often an illusion.\nI’ve spent the past year building a multi-provider LLM backend, and the pattern is always the same: things work for basic text generation, then break the moment you need production-critical features.\nThis analysis focuses on the /chat/completions endpoint, but the same fragmentation applies to /images/generations and /embeddings. As new agent-focused APIs emerge (like OpenAI’s stateful responses API, or Anthropic’s agent capabilities including code execution, MCP connector, and Files API), the risk of further fragmentation only grows.\nWhere compatibility breaks Structured output OpenAI lets you pass a JSON schema in response_format and the model conforms to it.\nAnthropic now supports structured output, but only for newer models (Sonnet 4.5, Opus 4.1, Haiku 4.5 as of December 2025). For older models like Claude 4 Sonnet, you need the tool-calling workaround: define a fake tool with your desired schema, force the model to “call” it, extract the arguments. If you’re running a production system with multiple Claude model versions, you need different code paths depending on which model handles the request.\nGemini claims to support structured output through their compatible API but frequently produces non-compliant results. Their native API handles this correctly; the “compatible” layer introduces the problem.\nQwen models recently added structured output support with schema enforcement.\nTool calling Tool calling is the backbone of modern agentic systems, and its implementation varies wildly across providers.\nOne of the most useful optimizations is streaming tool arguments as they’re generated. This allows an agent to begin preparing for tool execution before the model finishes outputting all its arguments, helping to reduce perceived latency. Support for this is inconsistent:\nOpenAI handles this cleanly. Anthropic only recently added support via a versioned beta header (anthropic-beta: fine-grained-tool-streaming-2025-05-14). Gemini doesn’t support it. Tool calls are buffered and sent in a single chunk at the end of the stream.\nThe JSON schema for defining tools is another problem. Every provider documents which parts of the JSON Schema specification they support, but some limitations are undocumented. Gemini imposes a depth limit on tool definition schemas that isn’t mentioned anywhere. You discover it when complex tools fail with unhelpful error messages.\nPrompt caching Prompt caching is beneficial for managing cost and latency, but implementations differ in ways that matter.\nOpenAI made caching automatic in October 2024. No code changes required for GPT-4o and newer models. Prompts over 1,024 tokens are cached transparently. Gemini also has automatic caching enabled by default.\nAnthropic requires manual cache control. You need to add cache_control parameters to mark which parts of your prompt should be cached, with explicit breakpoints and a 5-minute TTL that you must manage. Running Claude on Bedrock? Same manual approach, different SDK. Different approach, different code.\nBuilding a unified caching strategy across providers isn’t practical. You either write provider-specific caching logic or leave performance and cost savings on the table. (Note: I wrote the first draft of this 3-4 months ago and already things have changed a lot and I’m lazy to validate all the details.)\nReasoning traces Approaches to reasoning tokens vary significantly. Gemini has “adaptive thought” enabled by default. OpenAI doesn’t expose reasoning tokens at all, which complicates cost tracking across providers. Both Anthropic and Gemini have “thought signatures” that must be preserved in chat history. Fail to do so and you get cryptic validation errors.\nChat history structure Even the fundamental structure of chat history differs across native APIs.\nOpenAI uses three roles: user, assistant, and tool. Tool uses appear in assistant messages; results go in dedicated tool messages. Anthropic uses only user and assistant, with tool results as special content blocks within assistant messages. Gemini follows Anthropic’s pattern but renames assistant to model.\nThis makes chat history management and token tracking across providers unnecessarily complex.\nIt keeps changing Everything I’ve described above is a snapshot. By the time you read this, some of it will be outdated.\nAnthropic added structured output support in November 2025, but only for new models. OpenAI made caching automatic in October 2024. Gemini’s tool streaming behavior has changed multiple times. Bedrock’s OpenAI-compatible endpoint scope keeps evolving. Gemini has been hinting at tool calling support with structured output for a while.\nThis is the real problem. You build abstractions that handle today’s incompatibilities, then a provider ships a feature, changes their streaming format, or error codes. Your “unified” client becomes a maintenance nightmare.\nThe fragmentation isn’t static. It’s actively getting worse as providers race to ship features without coordinating on interfaces.\nRate limits Rate limiting sounds straightforward. TPM and RPM numbers that tell you how much capacity you have. In practice, every provider handles this differently, and the numbers alone don’t tell you what you need to know.\nOpenAI is generous with limits at higher tiers. But when load is high, they queue your requests instead of rejecting them. Your request sits in a queue, eating into your timeout budget, and you have no idea whether it will complete in 2 seconds or 20. For fallback strategies, this is a nightmare. You’re waiting on a provider that might never respond in time, instead of failing fast and routing elsewhere.\nGemini takes the opposite approach. Hit a limit, get a 429 immediately. For production systems with fallback logic, this is actually preferable. You know instantly to try another provider. But getting guaranteed capacity requires Provisioned Throughput on Vertex AI, which is expensive and requires upfront commitment.\nAnthropic has its own mechanism: a service_tier parameter you pass in API calls. Set it to “auto” and requests use Priority Tier capacity when available, falling back to standard. The response tells you which tier handled your request. It works, but it’s yet another provider-specific parameter to manage.\nThen there’s the cloud provider layer. Running Claude on Bedrock? Different rate limits than Anthropic’s native API. OpenAI on Azure? Same story. Each hosting platform adds its own quota system on top of the model provider’s limits.\nNone of this is discoverable from API docs alone. The queuing-vs-rejection behavior difference between OpenAI and Gemini? I learned that from production incidents. Whether you prefer fast failure or patient queuing depends on your use case, but you can’t make that choice if you don’t know how each provider behaves under pressure.\nSame model, different behavior The same model can behave differently depending on which platform hosts it.\nUsing an Anthropic model via AWS Bedrock versus Anthropic’s native API should be a simple switch. It isn’t. For a period, Bedrock’s streaming implementation had a bug: requests with invalid payloads and stream=True would be accepted, start streaming, then fail mid-stream with a server-side validation error. Anthropic’s native API rejected these upfront. Same model, different failure mode, different error handling required.\nBedrock recently added an OpenAI-compatible API, but it only applies to OpenAI models they host. To use Claude through Bedrock, you must abandon the compatible endpoint and use the boto3 SDK directly. So much for unified interfaces.\nThe integration overhead This lack of true compatibility creates a maintenance burden across the entire LLM ecosystem.\nThe result is a fragmented landscape of SDKs: native provider SDKs (openai, anthropic, google-genai), abstraction layers and agent frameworks (LangChain, Pydantic AI), and compatibility shims (LiteLLM, Mozilla AI’s any-llm). These compatibility layers spend enormous engineering effort patching over inconsistencies. They’re solving a problem that shouldn’t exist.\nLiteLLM deserves particular mention. It’s the most popular compatibility layer, but it’s built on a flawed foundation: rather than leveraging official provider SDKs, it reimplements provider interfaces using OpenAI-compatible parameters. This means LiteLLM is itself subject to the same compatibility assumptions it’s trying to abstract away. When a provider’s “compatible” endpoint diverges from OpenAI’s behavior, LiteLLM inherits the problem. You’re adding an abstraction layer that doesn’t actually insulate you from the underlying fragmentation.\nMozilla AI’s any-llm takes a different approach, wrapping official SDKs rather than reimplementing them. But the fact that we need multiple competing solutions to this problem underscores how broken the current state is.\nFor production systems that need multi-provider strategies (task-based routing, quota management, load balancing, A/B testing, fallbacks), each API inconsistency adds complexity to routing logic. The burden extends to observability providers like Arize Phoenix and Langfuse, who must build and maintain bespoke integrations for each provider.\nWhat might actually help? The solution isn’t more sophisticated compatibility layers. It’s a formal, open standard.\nA versioned specification. A public schema for request/response formats covering /chat/completions, /embeddings, and /images/generations. This would define the messages structure, tools behavior, and streaming formats unambiguously.\nA capabilities discovery endpoint. Instead of discovering limitations through trial and error, a standard endpoint (e.g., /capabilities) would let SDKs programmatically query supported features. It could return data like supports_tool_streaming: true, json_schema_compliance: \"draft-2020-12\", or max_tool_schema_depth: 4. SDKs could handle limitations gracefully rather than failing unexpectedly.\nStandardized error codes. A defined set of error codes for common LLM failures (content_filter_violation, tool_schema_unsupported, max_context_exceeded) would make error handling consistent across providers.\nA compliance test suite. For “compatibility” to mean something, providers claiming compliance should be verifiable against an open-source test suite.\nStandardized rate limit behavior. A way to query current limits, remaining quota, and specify whether a provider fast-fails or queues requests when approaching limits. This would let multi-provider systems make intelligent routing decisions.\nWho could fix this? Anthropic donated the Model Context Protocol (MCP) to the Agentic AI Foundation (AAIF) under the Linux Foundation. OpenAI contributed AGENTS.md, Block contributed goose. The foundation has backing from Google, Microsoft, AWS, Cloudflare, and Bloomberg.\nMCP solved a similar problem (connecting AI models to external tools and data sources) by creating an open, vendor-neutral standard. Within a year it went from internal Anthropic project to industry-wide adoption with millions of monthly SDK downloads.\nThe AAIF shows that major providers can collaborate on shared infrastructure when there’s a neutral governance structure. The /chat/completions API and its associated behaviors (tool calling, structured output, streaming) are at least as foundational as MCP. Arguably more so, since every LLM application touches these interfaces.\nIf the same players who formed the AAIF turned their attention to formalizing the completion API specification, they’d have both the credibility and the organizational structure to make it happen. The Linux Foundation has decades of experience stewarding critical infrastructure like Kubernetes and Node.js.\nThis seems like a natural extension of what’s already being done for tool connectivity.\nTL;DR “OpenAI-compatible” currently means “similar enough to seem interchangeable until it’s not.” Every serious system I’ve encountered ends up with provider-specific handling anyway.\nThe overhead from this fragmentation is real. A formal standard would let SDK authors focus on higher-level tooling instead of patching low-level differences. It would give developers genuine confidence when switching providers. And importantly, it would expand LLM usage beyond the Python/TypeScript ecosystem. Other languages could build robust clients against a stable specification instead of chasing moving targets.\nUntil then, don’t trust the compatibility claims. Use the provider’s native API if you’re serious about production.\n","wordCount":"1836","inLanguage":"en","datePublished":"2025-12-17T18:00:00+05:30","dateModified":"2025-12-17T18:00:00+05:30","author":{"@type":"Person","name":"Deepankar Mahapatro"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepankarm.github.io/posts/openai-compatibility-paradox/"},"publisher":{"@type":"Organization","name":"deepankar.log","logo":{"@type":"ImageObject","url":"https://deepankarm.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://deepankarm.github.io/ accesskey=h title="deepankar.log (Alt + H)">deepankar.log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://deepankarm.github.io/ title=Home><span>Home</span></a></li><li><a href=https://deepankarm.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://deepankarm.github.io/about/ title=About><span>About</span></a></li><li><a href=https://github.com/deepankarm title=GitHub><span>GitHub</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.linkedin.com/in/deepankar-mahapatro/ title=LinkedIn><span>LinkedIn</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The OpenAI Compatibility Paradox</h1><div class=post-description>Why the LLM API space needs a true standard</div><div class=post-meta><span title='2025-12-17 18:00:00 +0530 +0530'>December 17, 2025</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>Deepankar Mahapatro</span></div></header><div class=post-content><p>The promise of a standardized interface for LLMs via OpenAI-compatible endpoints is compelling. In theory, it allows for a plug-and-play architecture where switching models is as trivial as changing a <code>base_url</code>. In practice, this compatibility is often an illusion.</p><p>I&rsquo;ve spent the past year building a multi-provider LLM backend, and the pattern is always the same: things work for basic text generation, then break the moment you need production-critical features.</p><p>This analysis focuses on the <code>/chat/completions</code> endpoint, but the same fragmentation applies to <code>/images/generations</code> and <code>/embeddings</code>. As new agent-focused APIs emerge (like OpenAI&rsquo;s stateful <code>responses</code> API, or Anthropic&rsquo;s agent capabilities including code execution, MCP connector, and Files API), the risk of further fragmentation only grows.</p><hr><h2 id=where-compatibility-breaks>Where compatibility breaks<a hidden class=anchor aria-hidden=true href=#where-compatibility-breaks>#</a></h2><h3 id=structured-output>Structured output<a hidden class=anchor aria-hidden=true href=#structured-output>#</a></h3><p>OpenAI lets you pass a JSON schema in <code>response_format</code> and the model conforms to it.</p><p>Anthropic now supports structured output, but only for newer models (Sonnet 4.5, Opus 4.1, Haiku 4.5 as of December 2025). For older models like Claude 4 Sonnet, you need the tool-calling workaround: define a fake tool with your desired schema, force the model to &ldquo;call&rdquo; it, extract the arguments. If you&rsquo;re running a production system with multiple Claude model versions, you need different code paths depending on which model handles the request.</p><p>Gemini claims to support structured output through their compatible API but frequently produces non-compliant results. Their native API handles this correctly; the &ldquo;compatible&rdquo; layer introduces the problem.</p><p>Qwen models recently added structured output support with schema enforcement.</p><h3 id=tool-calling>Tool calling<a hidden class=anchor aria-hidden=true href=#tool-calling>#</a></h3><p>Tool calling is the backbone of modern agentic systems, and its implementation varies wildly across providers.</p><p>One of the most useful optimizations is streaming tool arguments as they&rsquo;re generated. This allows an agent to begin preparing for tool execution before the model finishes outputting all its arguments, helping to reduce perceived latency. Support for this is inconsistent:</p><p>OpenAI handles this cleanly. Anthropic only recently added support via a versioned beta header (<code>anthropic-beta: fine-grained-tool-streaming-2025-05-14</code>). Gemini doesn&rsquo;t support it. Tool calls are buffered and sent in a single chunk at the end of the stream.</p><p>The JSON schema for defining tools is another problem. Every provider documents which parts of the JSON Schema specification they support, but some limitations are undocumented. Gemini imposes a depth limit on tool definition schemas that isn&rsquo;t mentioned anywhere. You discover it when complex tools fail with unhelpful error messages.</p><h3 id=prompt-caching>Prompt caching<a hidden class=anchor aria-hidden=true href=#prompt-caching>#</a></h3><p>Prompt caching is beneficial for managing cost and latency, but implementations differ in ways that matter.</p><p>OpenAI made caching automatic in October 2024. No code changes required for GPT-4o and newer models. Prompts over 1,024 tokens are cached transparently. Gemini also has automatic caching enabled by default.</p><p>Anthropic requires manual cache control. You need to add <code>cache_control</code> parameters to mark which parts of your prompt should be cached, with explicit breakpoints and a 5-minute TTL that you must manage. Running Claude on Bedrock? Same manual approach, different SDK. Different approach, different code.</p><p>Building a unified caching strategy across providers isn&rsquo;t practical. You either write provider-specific caching logic or leave performance and cost savings on the table. (Note: I wrote the first draft of this 3-4 months ago and already things have changed a lot and I&rsquo;m lazy to validate all the details.)</p><h3 id=reasoning-traces>Reasoning traces<a hidden class=anchor aria-hidden=true href=#reasoning-traces>#</a></h3><p>Approaches to reasoning tokens vary significantly. Gemini has &ldquo;adaptive thought&rdquo; enabled by default. OpenAI doesn&rsquo;t expose reasoning tokens at all, which complicates cost tracking across providers. Both Anthropic and Gemini have &ldquo;thought signatures&rdquo; that must be preserved in chat history. Fail to do so and you get cryptic validation errors.</p><h3 id=chat-history-structure>Chat history structure<a hidden class=anchor aria-hidden=true href=#chat-history-structure>#</a></h3><p>Even the fundamental structure of chat history differs across native APIs.</p><p>OpenAI uses three roles: <code>user</code>, <code>assistant</code>, and <code>tool</code>. Tool uses appear in assistant messages; results go in dedicated <code>tool</code> messages. Anthropic uses only <code>user</code> and <code>assistant</code>, with tool results as special content blocks within assistant messages. Gemini follows Anthropic&rsquo;s pattern but renames <code>assistant</code> to <code>model</code>.</p><p>This makes chat history management and token tracking across providers unnecessarily complex.</p><h3 id=it-keeps-changing>It keeps changing<a hidden class=anchor aria-hidden=true href=#it-keeps-changing>#</a></h3><p>Everything I&rsquo;ve described above is a snapshot. By the time you read this, some of it will be outdated.</p><p>Anthropic added structured output support in November 2025, but only for new models. OpenAI made caching automatic in October 2024. Gemini&rsquo;s tool streaming behavior has changed multiple times. Bedrock&rsquo;s OpenAI-compatible endpoint scope keeps evolving. Gemini has been hinting at tool calling support with structured output for a while.</p><p>This is the real problem. You build abstractions that handle today&rsquo;s incompatibilities, then a provider ships a feature, changes their streaming format, or error codes. Your &ldquo;unified&rdquo; client becomes a maintenance nightmare.</p><p>The fragmentation isn&rsquo;t static. It&rsquo;s actively getting worse as providers race to ship features without coordinating on interfaces.</p><h3 id=rate-limits>Rate limits<a hidden class=anchor aria-hidden=true href=#rate-limits>#</a></h3><p>Rate limiting sounds straightforward. TPM and RPM numbers that tell you how much capacity you have. In practice, every provider handles this differently, and the numbers alone don&rsquo;t tell you what you need to know.</p><p>OpenAI is generous with limits at higher tiers. But when load is high, they queue your requests instead of rejecting them. Your request sits in a queue, eating into your timeout budget, and you have no idea whether it will complete in 2 seconds or 20. For fallback strategies, this is a nightmare. You&rsquo;re waiting on a provider that might never respond in time, instead of failing fast and routing elsewhere.</p><p>Gemini takes the opposite approach. Hit a limit, get a 429 immediately. For production systems with fallback logic, this is actually preferable. You know instantly to try another provider. But getting guaranteed capacity requires Provisioned Throughput on Vertex AI, which is expensive and requires upfront commitment.</p><p>Anthropic has its own mechanism: a <code>service_tier</code> parameter you pass in API calls. Set it to &ldquo;auto&rdquo; and requests use Priority Tier capacity when available, falling back to standard. The response tells you which tier handled your request. It works, but it&rsquo;s yet another provider-specific parameter to manage.</p><p>Then there&rsquo;s the cloud provider layer. Running Claude on Bedrock? Different rate limits than Anthropic&rsquo;s native API. OpenAI on Azure? Same story. Each hosting platform adds its own quota system on top of the model provider&rsquo;s limits.</p><p>None of this is discoverable from API docs alone. The queuing-vs-rejection behavior difference between OpenAI and Gemini? I learned that from production incidents. Whether you prefer fast failure or patient queuing depends on your use case, but you can&rsquo;t make that choice if you don&rsquo;t know how each provider behaves under pressure.</p><hr><h2 id=same-model-different-behavior>Same model, different behavior<a hidden class=anchor aria-hidden=true href=#same-model-different-behavior>#</a></h2><p>The same model can behave differently depending on which platform hosts it.</p><p>Using an Anthropic model via AWS Bedrock versus Anthropic&rsquo;s native API should be a simple switch. It isn&rsquo;t. For a period, Bedrock&rsquo;s streaming implementation had a bug: requests with invalid payloads and <code>stream=True</code> would be accepted, start streaming, then fail mid-stream with a server-side validation error. Anthropic&rsquo;s native API rejected these upfront. Same model, different failure mode, different error handling required.</p><p>Bedrock recently added an OpenAI-compatible API, but it only applies to OpenAI models they host. To use Claude through Bedrock, you must abandon the compatible endpoint and use the <code>boto3</code> SDK directly. So much for unified interfaces.</p><hr><h2 id=the-integration-overhead>The integration overhead<a hidden class=anchor aria-hidden=true href=#the-integration-overhead>#</a></h2><p>This lack of true compatibility creates a maintenance burden across the entire LLM ecosystem.</p><p>The result is a fragmented landscape of SDKs: native provider SDKs (<code>openai</code>, <code>anthropic</code>, <code>google-genai</code>), abstraction layers and agent frameworks (LangChain, Pydantic AI), and compatibility shims (LiteLLM, Mozilla AI&rsquo;s <a href=https://github.com/mozilla-ai/any-llm>any-llm</a>). These compatibility layers spend enormous engineering effort patching over inconsistencies. They&rsquo;re solving a problem that shouldn&rsquo;t exist.</p><p>LiteLLM deserves particular mention. It&rsquo;s the most popular compatibility layer, but it&rsquo;s built on a flawed foundation: rather than leveraging official provider SDKs, it reimplements provider interfaces using OpenAI-compatible parameters. This means LiteLLM is itself subject to the same compatibility assumptions it&rsquo;s trying to abstract away. When a provider&rsquo;s &ldquo;compatible&rdquo; endpoint diverges from OpenAI&rsquo;s behavior, LiteLLM inherits the problem. You&rsquo;re adding an abstraction layer that doesn&rsquo;t actually insulate you from the underlying fragmentation.</p><p>Mozilla AI&rsquo;s any-llm takes a different approach, wrapping official SDKs rather than reimplementing them. But the fact that we need multiple competing solutions to this problem underscores how broken the current state is.</p><p>For production systems that need multi-provider strategies (task-based routing, quota management, load balancing, A/B testing, fallbacks), each API inconsistency adds complexity to routing logic. The burden extends to observability providers like Arize Phoenix and Langfuse, who must build and maintain bespoke integrations for each provider.</p><hr><h2 id=what-might-actually-help>What might actually help?<a hidden class=anchor aria-hidden=true href=#what-might-actually-help>#</a></h2><p>The solution isn&rsquo;t more sophisticated compatibility layers. It&rsquo;s a formal, open standard.</p><p><strong>A versioned specification.</strong> A public schema for request/response formats covering <code>/chat/completions</code>, <code>/embeddings</code>, and <code>/images/generations</code>. This would define the <code>messages</code> structure, <code>tools</code> behavior, and streaming formats unambiguously.</p><p><strong>A capabilities discovery endpoint.</strong> Instead of discovering limitations through trial and error, a standard endpoint (e.g., <code>/capabilities</code>) would let SDKs programmatically query supported features. It could return data like <code>supports_tool_streaming: true</code>, <code>json_schema_compliance: "draft-2020-12"</code>, or <code>max_tool_schema_depth: 4</code>. SDKs could handle limitations gracefully rather than failing unexpectedly.</p><p><strong>Standardized error codes.</strong> A defined set of error codes for common LLM failures (<code>content_filter_violation</code>, <code>tool_schema_unsupported</code>, <code>max_context_exceeded</code>) would make error handling consistent across providers.</p><p><strong>A compliance test suite.</strong> For &ldquo;compatibility&rdquo; to mean something, providers claiming compliance should be verifiable against an open-source test suite.</p><p><strong>Standardized rate limit behavior.</strong> A way to query current limits, remaining quota, and specify whether a provider fast-fails or queues requests when approaching limits. This would let multi-provider systems make intelligent routing decisions.</p><hr><h2 id=who-could-fix-this>Who could fix this?<a hidden class=anchor aria-hidden=true href=#who-could-fix-this>#</a></h2><p>Anthropic donated the Model Context Protocol (MCP) to the Agentic AI Foundation (AAIF) under the Linux Foundation. OpenAI contributed AGENTS.md, Block contributed goose. The foundation has backing from Google, Microsoft, AWS, Cloudflare, and Bloomberg.</p><p>MCP solved a similar problem (connecting AI models to external tools and data sources) by creating an open, vendor-neutral standard. Within a year it went from internal Anthropic project to industry-wide adoption with millions of monthly SDK downloads.</p><p>The AAIF shows that major providers can collaborate on shared infrastructure when there&rsquo;s a neutral governance structure. The <code>/chat/completions</code> API and its associated behaviors (tool calling, structured output, streaming) are at least as foundational as MCP. Arguably more so, since every LLM application touches these interfaces.</p><p>If the same players who formed the AAIF turned their attention to formalizing the completion API specification, they&rsquo;d have both the credibility and the organizational structure to make it happen. The Linux Foundation has decades of experience stewarding critical infrastructure like Kubernetes and Node.js.</p><p>This seems like a natural extension of what&rsquo;s already being done for tool connectivity.</p><hr><h2 id=tldr>TL;DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>&ldquo;OpenAI-compatible&rdquo; currently means &ldquo;similar enough to seem interchangeable until it&rsquo;s not.&rdquo; Every serious system I&rsquo;ve encountered ends up with provider-specific handling anyway.</p><p>The overhead from this fragmentation is real. A formal standard would let SDK authors focus on higher-level tooling instead of patching low-level differences. It would give developers genuine confidence when switching providers. And importantly, it would expand LLM usage beyond the Python/TypeScript ecosystem. Other languages could build robust clients against a stable specification instead of chasing moving targets.</p><p>Until then, don&rsquo;t trust the compatibility claims. Use the provider&rsquo;s native API if you&rsquo;re serious about production.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://deepankarm.github.io/tags/llm/>LLM</a></li><li><a href=https://deepankarm.github.io/tags/openai/>OpenAI</a></li><li><a href=https://deepankarm.github.io/tags/anthropic/>Anthropic</a></li><li><a href=https://deepankarm.github.io/tags/gemini/>Gemini</a></li><li><a href=https://deepankarm.github.io/tags/completions-api/>Completions API</a></li></ul><nav class=paginav><a class=next href=https://deepankarm.github.io/posts/streaming-partial-json-llm/><span class=title>Next »</span><br><span>Streaming Partial JSON from LLMs in Go</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://deepankarm.github.io/>deepankar.log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>